{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vh2w2JZ3wH5N"
   },
   "source": [
    "# Heart Disease Detection\n",
    "\n",
    "    Referred Paper:\n",
    "    https://pmc.ncbi.nlm.nih.gov/articles/PMC9687844/#_ad93_\n",
    "\n",
    "    Referred MRI Data Set:\n",
    "    https://www.kaggle.com/datasets/redwankarimsony/heart-disease-data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EKlCtMkAwH5O"
   },
   "source": [
    "Author's Model Replication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "ZiC2jZ-AwH5O"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc\n",
    "from sklearn.utils import class_weight\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lm8byOAMwH5P"
   },
   "source": [
    "## Importing Heart Diseases Data and reshaping dataset for model [Filled NA values with Median]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 373
    },
    "id": "ysYpQWDjwH5P",
    "outputId": "3b45f695-5284-4cbb-fd21-4fcc539aaf49"
   },
   "outputs": [],
   "source": [
    "# Load dataset (UCI Heart Disease Dataset transformed)\n",
    "file_path = \"data.csv\"\n",
    "columns = [\"age\", \"sex\", \"cp\", \"trestbps\", \"chol\", \"fbs\", \"restecg\", \"thalach\", \"exang\", \"oldpeak\", \"slope\", \"ca\", \"thal\", \"target\"]\n",
    "# df = pd.read_csv(url, names=columns, na_values='?')\n",
    "df = pd.read_csv(file_path, names=columns, na_values='?')\n",
    "\n",
    "# Replace nan values with mean\n",
    "df_mean = df.median()\n",
    "df.fillna(df_mean, inplace=True)\n",
    "# Handle missing values\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# Convert categorical features\n",
    "label_encoders = {}\n",
    "for col in ['ca', 'thal']:\n",
    "    le = LabelEncoder()\n",
    "    df[col] = le.fit_transform(df[col])\n",
    "    label_encoders[col] = le\n",
    "\n",
    "# Convert target variable (binary classification: 0 (no disease) vs. 1 (disease))\n",
    "df['target'] = (df['target'] > 0).astype(int)\n",
    "\n",
    "# Split dataset\n",
    "X = df.drop('target', axis=1)\n",
    "y = df['target']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1fq7sak8wH5Q"
   },
   "outputs": [],
   "source": [
    "# Print data shape\n",
    "print(\"Training Data Shape:\", X_train.shape)\n",
    "print(\"Testing Data Shape:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bT8OtHpfxTLA"
   },
   "source": [
    "Creating Author's 1D Deep CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qW01eynawH5Q"
   },
   "outputs": [],
   "source": [
    "# Model V1 (As per the paper DCNN architecture)\n",
    "def build_model_v1():\n",
    "\n",
    "    dropout_rate = 0.03  # 3% dropout\n",
    "\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Input(shape=(13, 1)),  # Adjusted to 13 features\n",
    "        tf.keras.layers.Conv1D(32, kernel_size=3, activation='elu'),\n",
    "        tf.keras.layers.MaxPooling1D(pool_size=2),\n",
    "        tf.keras.layers.Conv1D(64, kernel_size=3, activation='elu'),\n",
    "        tf.keras.layers.MaxPooling1D(pool_size=2),\n",
    "        tf.keras.layers.Flatten(),\n",
    "\n",
    "        # Fully connected layers (4 layers with 128 neurons)\n",
    "        tf.keras.layers.Dense(128, activation='elu'),\n",
    "        tf.keras.layers.Dropout(dropout_rate),  # 3% dropout\n",
    "        tf.keras.layers.Dense(128, activation='elu'),\n",
    "        tf.keras.layers.Dropout(dropout_rate),  # 3% dropout\n",
    "        tf.keras.layers.Dense(128, activation='elu'),\n",
    "        tf.keras.layers.Dropout(dropout_rate),  # 3% dropout\n",
    "        tf.keras.layers.Dense(128, activation='elu'),\n",
    "        tf.keras.layers.Dropout(dropout_rate),  # 3% dropout\n",
    "\n",
    "        # Fully connected layers (3 layers with 64 neurons)\n",
    "        tf.keras.layers.Dense(64, activation='elu'),\n",
    "        tf.keras.layers.Dropout(dropout_rate),  # 3% dropout\n",
    "        tf.keras.layers.Dense(64, activation='elu'),\n",
    "        tf.keras.layers.Dropout(dropout_rate),  # 3% dropout\n",
    "        tf.keras.layers.Dense(64, activation='elu'),\n",
    "        tf.keras.layers.Dropout(dropout_rate),  # 3% dropout\n",
    "\n",
    "        # Output layer\n",
    "        tf.keras.layers.Dense(1, activation='sigmoid')  # Binary classification\n",
    "    ])\n",
    "\n",
    "    # Compile model with Nadam optimizer\n",
    "    model.compile(optimizer=tf.keras.optimizers.Nadam(learning_rate=0.001),\n",
    "                loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    print(\"Model Summary:\\n\")\n",
    "    model.summary()\n",
    "    print(\"\\n\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DuxqYXfNwH5R"
   },
   "source": [
    "Creating our own 1D CNN BMK Model (less complex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ajwvwn9cwH5R"
   },
   "outputs": [],
   "source": [
    "# Model V2 (RELU activation model)\n",
    "def build_model_v2():\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Input(shape=(13, 1)),  # Input layer with 13 features (1D)\n",
    "        tf.keras.layers.Conv1D(32, kernel_size=3, activation='relu'),  # 1D Convolutional layer with 32 filters\n",
    "        tf.keras.layers.MaxPooling1D(pool_size=2),  # Max pooling layer to reduce dimensionality\n",
    "\n",
    "        tf.keras.layers.Conv1D(64, kernel_size=3, activation='relu'),  # Another convolutional layer with 64 filters\n",
    "        tf.keras.layers.MaxPooling1D(pool_size=2),  # Another max pooling layer\n",
    "\n",
    "        tf.keras.layers.Flatten(),  # Flatten the 2D output into 1D\n",
    "        tf.keras.layers.Dense(128, activation='relu'),  # Fully connected layer with 128 neurons\n",
    "        tf.keras.layers.Dropout(0.05),  # Dropout layer to prevent overfitting\n",
    "        tf.keras.layers.Dense(1, activation='sigmoid')  # Output layer with sigmoid activation for binary classification\n",
    "    ])\n",
    "    # Compile the model with Nadam optimizer, binary crossentropy loss, and accuracy metric\n",
    "    # model.compile(optimizer=tf.keras.optimizers.Nadam(learning_rate=0.001),\n",
    "    #               loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "                loss='MeanSquaredError', metrics=['accuracy'])\n",
    "\n",
    "    print(\"Model Summary:\\n\")\n",
    "    model.summary()\n",
    "    print(\"\\n\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9OXhWHfQwH5R"
   },
   "source": [
    "Running Author's model with transformed data set where we replaced nan values with median."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2lNDnJZ6wH5R"
   },
   "outputs": [],
   "source": [
    "# Reshape input data for Conv1D\n",
    "X_train = X_train.reshape(-1, 13, 1)\n",
    "X_test = X_test.reshape(-1, 13, 1)\n",
    "\n",
    "# Train model\n",
    "model = build_model_v1()\n",
    "\n",
    "# to avoid overfitting\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "# history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=100, batch_size=16, verbose=1)\n",
    "# history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=100, batch_size=32, verbose=1, callbacks=[early_stopping])\n",
    "\n",
    "class_weight = {0: 1, 1: 10}  # Adjust based on your dataset\n",
    "history =  model.fit(X_train, y_train, epochs=100, batch_size=32, validation_data=(X_test, y_test), class_weight=class_weight)\n",
    "# history = model.fit(X_train, y_train, epochs=100, batch_size=32, validation_data=(X_test, y_test), verbose=1)\n",
    "\n",
    "# Evaluate model\n",
    "y_pred_prob = model.predict(X_test)\n",
    "y_pred = (y_pred_prob > 0.5).astype(int)\n",
    "# print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zjrY3Ix4wH5S"
   },
   "source": [
    "These functions are used to plot accuracies, loss, history and ROC Curves. These is also a function that handles the comparison logic between V1 and V2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PDn4i2WzwH5S"
   },
   "outputs": [],
   "source": [
    "# Plot training & validation loss\n",
    "def plot_training_loss(history):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(history.history['loss'], label='Train Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.title('Training vs. Validation Loss')\n",
    "    plt.tight_layout()\n",
    "    # plt.savefig('plots/loss_test.png')\n",
    "    plt.show()\n",
    "plot_training_loss(history)\n",
    "\n",
    "# Plot training & validation accuracy\n",
    "def plot_training_history(history):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.title('Training vs. Validation Accuracy')\n",
    "    plt.tight_layout()\n",
    "    # plt.savefig('plots/accuracy_test.png')\n",
    "    plt.show()\n",
    "\n",
    "# Confusion Matrix\n",
    "def plot_confusion_matrix(y_test, y_pred):\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['No Disease', 'Disease'], yticklabels=['No Disease', 'Disease'])\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.tight_layout()\n",
    "    # plt.savefig('plots/confusion_matrix_test.png')\n",
    "    plt.show()\n",
    "\n",
    "# Plot ROC Curve\n",
    "def plot_roc_curve(y_test, y_pred_prob):\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_pred_prob)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(fpr, tpr, color='blue', lw=2, label=f'ROC curve (area = {roc_auc*100:.2f})')\n",
    "    plt.plot([0, 1], [0, 1], color='gray', linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC Curve')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.tight_layout()\n",
    "    # plt.savefig('plots/roc_curve_test.png')\n",
    "    plt.show()\n",
    "\n",
    "# Function to plot the comparison of both models\n",
    "def plot_comparisons(history_v1, history_v2, y_test_v1, y_pred_v1, y_test_v2, y_pred_v2, y_pred_prob_v1, y_pred_prob_v2, save=True):\n",
    "    # Plot Loss Comparison\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(history_v1.history['loss'], label='V1 Train Loss', color='blue')\n",
    "    plt.plot(history_v1.history['val_loss'], label='V1 Val Loss', color='cyan', linestyle='--')\n",
    "    plt.plot(history_v2.history['loss'], label='V2 Train Loss', color='red')\n",
    "    plt.plot(history_v2.history['val_loss'], label='V2 Val Loss', color='orange', linestyle='--')\n",
    "    plt.title('Training and Validation Loss Comparison')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    if save:\n",
    "        plt.savefig('plots/loss_comparison.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "    # Plot Accuracy Comparison\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(history_v1.history['accuracy'], label='V1 Train Accuracy', color='blue')\n",
    "    plt.plot(history_v1.history['val_accuracy'], label='V1 Val Accuracy', color='cyan', linestyle='--')\n",
    "    plt.plot(history_v2.history['accuracy'], label='V2 Train Accuracy', color='red')\n",
    "    plt.plot(history_v2.history['val_accuracy'], label='V2 Val Accuracy', color='orange', linestyle='--')\n",
    "    plt.title('Training and Validation Accuracy Comparison')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    if save:\n",
    "        plt.savefig('plots/accuracy_comparison.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "    # Plot Confusion Matrix Comparison\n",
    "    cm_v1 = confusion_matrix(y_test_v1, y_pred_v1)\n",
    "    cm_v2 = confusion_matrix(y_test_v2, y_pred_v2)\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(14, 6))\n",
    "    sns.heatmap(cm_v1, annot=True, fmt='d', cmap='Blues', xticklabels=['No Disease', 'Disease'], yticklabels=['No Disease', 'Disease'], ax=axs[0])\n",
    "    axs[0].set_title('Model V1 Confusion Matrix')\n",
    "    axs[0].set_xlabel('Predicted')\n",
    "    axs[0].set_ylabel('Actual')\n",
    "    sns.heatmap(cm_v2, annot=True, fmt='d', cmap='Blues', xticklabels=['No Disease', 'Disease'], yticklabels=['No Disease', 'Disease'], ax=axs[1])\n",
    "    axs[1].set_title('Model V2 Confusion Matrix')\n",
    "    axs[1].set_xlabel('Predicted')\n",
    "    axs[1].set_ylabel('Actual')\n",
    "    plt.tight_layout()\n",
    "    if save:\n",
    "        plt.savefig('plots/confusion_matrix_comparison.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "    # Plot ROC Curve Comparison\n",
    "    fpr_v1, tpr_v1, _ = roc_curve(y_test_v1, y_pred_prob_v1)\n",
    "    fpr_v2, tpr_v2, _ = roc_curve(y_test_v2, y_pred_prob_v2)\n",
    "    roc_auc_v1 = auc(fpr_v1, tpr_v1)\n",
    "    roc_auc_v2 = auc(fpr_v2, tpr_v2)\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(fpr_v1, tpr_v1, color='blue', lw=2, label=f'V1 ROC curve (area = {roc_auc_v1*100:.2f}%)')\n",
    "    plt.plot(fpr_v2, tpr_v2, color='red', lw=2, label=f'V2 ROC curve (area = {roc_auc_v2*100:.2f}%)')\n",
    "    plt.plot([0, 1], [0, 1], color='gray', linestyle='--')\n",
    "    plt.title('ROC Curve Comparison')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.tight_layout()\n",
    "    if save:\n",
    "        plt.savefig('plots/roc_curve_comparison.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "go1-_N-2wH5S"
   },
   "source": [
    "The performance metrics function creates a report for out V1 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AmVSW0V9wH5S"
   },
   "outputs": [],
   "source": [
    "def performance_metrics(y_test, y_pred):\n",
    "    # Generate classification report for the current model\n",
    "    report = classification_report(y_test, y_pred, output_dict=True)\n",
    "\n",
    "    # Extract overall metrics\n",
    "    metrics = {\n",
    "        'Accuracy': report['accuracy'],\n",
    "        'Precision': report['weighted avg']['precision'],\n",
    "        'Recall': report['weighted avg']['recall'],\n",
    "        'F1 Score': report['weighted avg']['f1-score']\n",
    "    }\n",
    "\n",
    "    # Create a DataFrame for metrics\n",
    "    metrics_df = pd.DataFrame(metrics, index=['Model V1']).T\n",
    "\n",
    "    # Export the metrics table to a CSV file\n",
    "    # metrics_df.to_csv('model_v1_metrics.csv', index=True)\n",
    "\n",
    "    # Print the metrics DataFrame\n",
    "    print(\"Model V1 Metrics:\\n\")\n",
    "    print(metrics_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "juHWJzcPwH5S"
   },
   "source": [
    "Finally plotting the properties and performance metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cHLsqs5dwH5T"
   },
   "outputs": [],
   "source": [
    "plot_training_history(history)\n",
    "plot_confusion_matrix(y_test, y_pred)\n",
    "plot_roc_curve(y_test, y_pred_prob)\n",
    "\n",
    "performance_metrics(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "khwuf--cwH5T"
   },
   "source": [
    "# Construction and Implementation of BMK Model (V2) and Comparing it with Author's (V1) Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BMK Model - Bhanuse, Majd, and Khanna model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MPcHskXzwH5T"
   },
   "source": [
    "Here we don't fill out the NaN values, we just remove all the empty or bad data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f7zftwmTwH5T"
   },
   "outputs": [],
   "source": [
    "# Re-Load the dataset (UCI Heart Disease Dataset)\n",
    "file_path = \"data/data.csv\"\n",
    "columns = [\"age\", \"sex\", \"cp\", \"trestbps\", \"chol\", \"fbs\", \"restecg\", \"thalach\", \"exang\", \"oldpeak\", \"slope\", \"ca\", \"thal\", \"target\"]\n",
    "df = pd.read_csv(file_path, names=columns, na_values='?')  # Replace '?' with NaN\n",
    "\n",
    "# Handle missing values by dropping rows with any NaN values\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# Convert categorical features ('ca' and 'thal') to numeric using Label Encoding\n",
    "label_encoders = {}\n",
    "for col in ['ca', 'thal']:\n",
    "    le = LabelEncoder()  # Initialize label encoder\n",
    "    df[col] = le.fit_transform(df[col])  # Convert categorical labels to numeric\n",
    "    label_encoders[col] = le  # Store the encoder for future use\n",
    "\n",
    "# Convert target variable to binary classification (0 = no disease, 1 = disease)\n",
    "df['target'] = (df['target'] > 0).astype(int)  # Convert to 0 or 1\n",
    "\n",
    "# Split the dataset into features (X) and target (y)\n",
    "X = df.drop('target', axis=1)  # Features (drop the 'target' column)\n",
    "y = df['target']  # Target variable (disease/no disease)\n",
    "\n",
    "# Split the dataset into training and testing sets (80% training, 20% testing)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Standardize the features by removing the mean and scaling to unit variance\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)  # Fit and transform the training data\n",
    "X_test = scaler.transform(X_test)  # Only transform the test data (use the training set's scaler)\n",
    "\n",
    "# Reshape input data for Conv1D: The model expects data of shape (samples, time steps, features)\n",
    "X_train = X_train.reshape(-1, 13, 1)  # Reshape training data to 3D\n",
    "X_test = X_test.reshape(-1, 13, 1)  # Reshape test data to 3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7CT5IRLNwH5T"
   },
   "outputs": [],
   "source": [
    "# Print the shapes of the training and testing data\n",
    "print(\"Training Data Shape:\", X_train.shape)\n",
    "print(\"Testing Data Shape:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BHwvPUYUwH5T"
   },
   "source": [
    "This function has the logic for getting a model as a function parameter and then it trains and runs the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N_Q1OtfBwH5T"
   },
   "outputs": [],
   "source": [
    "# Function to train and evaluate a model\n",
    "def train_and_evaluate_model(model_func):\n",
    "    # Initialize model\n",
    "    model = model_func()\n",
    "\n",
    "    # Train the model\n",
    "    history = model.fit(X_train, y_train, epochs=100, batch_size=32, validation_data=(X_test, y_test), verbose=1, class_weight={0: 1, 1: 10})\n",
    "\n",
    "    # Predict probabilities and threshold for binary predictions\n",
    "    y_pred_prob = model.predict(X_test)\n",
    "    y_pred = (y_pred_prob > 0.5).astype(int)\n",
    "\n",
    "    return history, y_pred, y_pred_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UN_gax_zwH5U"
   },
   "source": [
    "This function handles the logic to compare the performance metrics of V1 (Author's) and V2 (Our Own) Models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ut1rPQWNwH5U"
   },
   "outputs": [],
   "source": [
    "def compare_performance_metrics(y_test, y_pred_v1, y_pred_v2):\n",
    "    # Generate classification reports for both models\n",
    "    report_v1 = classification_report(y_test, y_pred_v1, output_dict=True)\n",
    "    report_v2 = classification_report(y_test, y_pred_v2, output_dict=True)\n",
    "\n",
    "    # Extract overall metrics for both models\n",
    "    metrics_v1 = {\n",
    "        'Accuracy': report_v1['accuracy'],\n",
    "        'Precision': report_v1['weighted avg']['precision'],\n",
    "        'Recall': report_v1['weighted avg']['recall'],\n",
    "        'F1 Score': report_v1['weighted avg']['f1-score']\n",
    "    }\n",
    "    metrics_v2 = {\n",
    "        'Accuracy': report_v2['accuracy'],\n",
    "        'Precision': report_v2['weighted avg']['precision'],\n",
    "        'Recall': report_v2['weighted avg']['recall'],\n",
    "        'F1 Score': report_v2['weighted avg']['f1-score']\n",
    "    }\n",
    "\n",
    "    # Create a DataFrame for comparison\n",
    "    comparison_df = pd.DataFrame([metrics_v1, metrics_v2], index=['Model V1', 'Model V2']).T\n",
    "\n",
    "    # Export the comparison table to a CSV file\n",
    "    comparison_df.to_csv('model_comparison.csv', index=True)\n",
    "\n",
    "    # Print the comparison DataFrame\n",
    "    # print(\"Comparison Table:\\n\")\n",
    "    display(comparison_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GJ6iyzJUwH5U"
   },
   "source": [
    "Finally, we train both the models. Plot the prediction results and performance metrics. Then finally compare them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LZHLRsjSwH5U"
   },
   "outputs": [],
   "source": [
    "# Train and evaluate both models\n",
    "history_v1, y_pred_v1, y_pred_prob_v1 = train_and_evaluate_model(build_model_v1)\n",
    "history_v2, y_pred_v2, y_pred_prob_v2 = train_and_evaluate_model(build_model_v2)\n",
    "\n",
    "# Plot comparisons for both models\n",
    "plot_comparisons(history_v1, history_v2, y_test, y_pred_v1, y_test, y_pred_v2, y_pred_prob_v1, y_pred_prob_v2, save=False)\n",
    "\n",
    "# Compare performance metrics for both models\n",
    "compare_performance_metrics(y_test, y_pred_v1, y_pred_v2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6yBd68gDdrej"
   },
   "source": [
    "Brain_MRI_Analysis using 2D and 1D CNN:\n",
    "\n",
    "Original file is located at\n",
    "    https://colab.research.google.com/drive/1dDWU8_tQ7pDdh3JnmmhJ3JtvdiIoszUP\n",
    "\n",
    "    Referred Paper:\n",
    "    https://bmcmedinformdecismak.biomedcentral.com/articles/10.1186/s12911-023-02114-6\n",
    "\n",
    "    Referred MRI Data Set:\n",
    "    https://www.kaggle.com/datasets/sartajbhuvaji/brain-tumor-classification-mri"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FVMpzsMJd4s9"
   },
   "source": [
    "First, we import all the dependencies that we need to perform the 2D CNN analysis on the Brain MRI Images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "zVhFxYZujD4J"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bq89L1g_eqOY"
   },
   "source": [
    "This is to unzip the images folder that we get from the Kaggle wesbite that we have mentioned above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "b2oHMZf-QbpU",
    "outputId": "572efcfe-800b-45d8-d0e6-a105e39357bb"
   },
   "outputs": [],
   "source": [
    "!unzip archive.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8IWPi8Q2e5Rv"
   },
   "source": [
    "This function rescales the images to 80*80 pixels, converts the image to grayscale and then add it to the image directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "vOyK9tFEe4ZB"
   },
   "outputs": [],
   "source": [
    "def process_image(jpg_image, image_directory, image_labels, type, label):\n",
    "    jpg_image = jpg_image.convert(\"L\")\n",
    "    jpg_resized_image = jpg_image.resize((80, 80))\n",
    "\n",
    "    # Convert to NumPy array\n",
    "    image_array = np.array(jpg_resized_image, dtype=np.float32)  # FITS prefers float32\n",
    "    # Normalization\n",
    "    image_array = image_array / 255\n",
    "\n",
    "    # rotated_image = np.rot90(image_array, axes=(1, 0))\n",
    "    # flipped_image = np.flipud(image_array)\n",
    "\n",
    "    image_directory[type].append(image_array)\n",
    "    image_labels.append(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mP-d5c3ZfY9q"
   },
   "source": [
    "This function imports all the images and then calls the process function for transformation and storing thema as NumPy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "id": "imy7MIBOPJNU"
   },
   "outputs": [],
   "source": [
    "# Load the JPG images\n",
    "def import_images(file_directories, class_directory, training_images_directory, training_labels):\n",
    "    # Loop through all files in the Training folder\n",
    "    for file in file_directories:\n",
    "        file_directory = \"Training/\" + file\n",
    "\n",
    "        for filename in os.listdir(file_directory):\n",
    "            file_path = os.path.join(file_directory, filename)\n",
    "\n",
    "            type = None\n",
    "            if 'glioma' in file_path:\n",
    "                type = 'glioma'\n",
    "            elif 'meningioma' in file_path:\n",
    "                type = 'meningioma'\n",
    "            elif 'pituitary' in file_path:\n",
    "                type = 'pituitary'\n",
    "            else:\n",
    "                type = 'healthy'\n",
    "\n",
    "            label = class_directory[type]\n",
    "\n",
    "            # Check if the file is a JPG image\n",
    "            if filename.lower().endswith('.jpg'):\n",
    "                # Open the image correctly with the full path\n",
    "                jpg_image = Image.open(file_path)  # Convert to grayscale\n",
    "\n",
    "                # Process image to rescale and then add it onto the training directory\n",
    "                process_image(jpg_image, training_images_directory, training_labels, type, label)\n",
    "\n",
    "    for file in file_directories:\n",
    "        file_directory = \"Testing/\" + file\n",
    "\n",
    "        for filename in os.listdir(file_directory):\n",
    "            file_path = os.path.join(file_directory, filename)\n",
    "\n",
    "            type = None\n",
    "            if 'glioma' in file_path:\n",
    "                type = 'glioma'\n",
    "            elif 'meningioma' in file_path:\n",
    "                type = 'meningioma'\n",
    "            elif 'pituitary' in file_path:\n",
    "                type = 'pituitary'\n",
    "            else:\n",
    "                type = 'healthy'\n",
    "\n",
    "            label = class_directory[type]\n",
    "\n",
    "            # Check if the file is a JPG image\n",
    "            if filename.lower().endswith('.jpg'):\n",
    "                jpg_image = Image.open(file_path)  # Convert to grayscale\n",
    "\n",
    "                # Process image to rescale and then add it onto the training directory\n",
    "                process_image(jpg_image, training_images_directory, training_labels, type, label)\n",
    "\n",
    "    return training_images_directory, training_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VQ760M3Ufmjv"
   },
   "source": [
    "This is the 2D CNN model that we created to process, train and then predict the tumors or no tumors from the MRI images that we have with us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "2dB-AwUxRd79"
   },
   "outputs": [],
   "source": [
    "# Building the 2D-CNN Model to train BRAIN MRI Images\n",
    "def build_model_2D_cnn():\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Input(shape=(80, 80, 1)),\n",
    "        tf.keras.layers.Conv2D(64, kernel_size=(2,2), activation='relu'),\n",
    "        tf.keras.layers.Conv2D(64, kernel_size=(2,2), activation='relu'),\n",
    "        tf.keras.layers.MaxPooling2D(pool_size=(2,2)),  # Max pooling layer to reduce dimensionality\n",
    "        tf.keras.layers.Dropout(0.1),  # Dropout layer to prevent overfitting\n",
    "\n",
    "        tf.keras.layers.Conv2D(32, kernel_size=(2,2), activation='relu'),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Conv2D(32, kernel_size=(2,2), activation='relu'),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.MaxPooling2D(pool_size=(2,2)),  # Max pooling layer to reduce dimensionality\n",
    "        tf.keras.layers.Dropout(0.1),  # Dropout layer to prevent overfitting\n",
    "\n",
    "        tf.keras.layers.Conv2D(16, kernel_size=(2,2), activation='relu'),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Conv2D(16, kernel_size=(2,2), activation='relu'),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.MaxPooling2D(pool_size=(2,2)),  # Max pooling layer to reduce dimensionality\n",
    "        tf.keras.layers.Dropout(0.1),  # Dropout layer to prevent overfitting\n",
    "\n",
    "        tf.keras.layers.Conv2D(8, kernel_size=(2,2), activation='relu'),\n",
    "        tf.keras.layers.Conv2D(8, kernel_size=(2,2), activation='relu'),\n",
    "        tf.keras.layers.MaxPooling2D(pool_size=(2,2)),  # Max pooling layer to reduce dimensionality\n",
    "        tf.keras.layers.Dropout(0.1),  # Dropout layer to prevent overfitting\n",
    "\n",
    "        # The dense layers\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(1024),                # No activation here!\n",
    "        tf.keras.layers.BatchNormalization(),      # Batch Normalization before activation\n",
    "        tf.keras.layers.Activation('relu'),\n",
    "        tf.keras.layers.Dropout(0.2),              # Dropout after activation\n",
    "        tf.keras.layers.Dense(4, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "                loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    print(\"Model V2 Summary:\\n\")\n",
    "    model.summary()\n",
    "    print(\"\\n\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OJj3TdOHf8bj"
   },
   "source": [
    "For our curiosity, we created a 1D CNN model similar to V2 model for heart disease to see how effectively that model can train compared with 2D CNN. Amazingly, we got very similar results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "id": "86yi-QMiUwXz"
   },
   "outputs": [],
   "source": [
    "# Building the 1D CNN Model to train BRAIN MRI Images\n",
    "def build_model_1D_cnn():\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Reshape((6400, 1), input_shape=(80, 80, 1)),  # Input layer reshaped to handle 1D 6400 pixel values of an image from 80 * 80 in 2D\n",
    "        tf.keras.layers.Conv1D(128, kernel_size=3, activation='relu'),  # Convolutional layer with 128 filters\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.MaxPooling1D(pool_size=2),  # Initial max pooling layer\n",
    "        tf.keras.layers.Dropout(0.1),\n",
    "\n",
    "        tf.keras.layers.Conv1D(64, kernel_size=3, activation='relu'),  # Another convolutional layer with 64 filters\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.MaxPooling1D(pool_size=2),  # Another max pooling layer\n",
    "        tf.keras.layers.Dropout(0.1),\n",
    "\n",
    "        tf.keras.layers.Conv1D(32, kernel_size=3, activation='relu'),   # Another convolutional layer with 64 filters\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.MaxPooling1D(pool_size=2),  # Max pooling layer to reduce dimensionality\n",
    "        tf.keras.layers.Dropout(0.1),\n",
    "\n",
    "        # The dense layers\n",
    "        tf.keras.layers.Flatten(),  # Flatten the 2D output into 1D\n",
    "        tf.keras.layers.Dense(128, activation='relu'),  # Fully connected layer with 128 neurons\n",
    "        tf.keras.layers.Dropout(0.2),  # Dropout layer to prevent overfitting\n",
    "        tf.keras.layers.Dense(4, activation='softmax')  # Output layer with softmax activation for multi-class classification\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "                loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    print(\"Model V2 Summary:\\n\")\n",
    "    model.summary()\n",
    "    print(\"\\n\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iHM1Se-PgPlH"
   },
   "source": [
    "This is the code that initialize the model and then finally performs training and testing on the images, and get predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "es8kkGv9gPI4"
   },
   "outputs": [],
   "source": [
    "def train_and_evaluate_model(model_func, training_data, training_label, testing_data, testing_label, class_weight):\n",
    "    # Initialize model\n",
    "    model = model_func()\n",
    "\n",
    "    # Train the model\n",
    "    history = model.fit(training_data, training_label, epochs=20, batch_size=16, validation_data=(testing_data, testing_label), verbose=1, class_weight = class_weights)\n",
    "\n",
    "    # Predict probabilities and threshold for multi-class predictions\n",
    "    y_pred_prob = model.predict(testing_data)\n",
    "\n",
    "    return history, y_pred_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kcBoN9Keged9"
   },
   "source": [
    "This snippet has all the plotting functions to visualize our accuracies, losses and ROC curves for both training and validations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "M8wtZhdMaQDh"
   },
   "outputs": [],
   "source": [
    "# Confusion matrix for multi-classification: Visualize true positives, false positives, true negatives, and false negatives\n",
    "def plot_confusion_matrix(y_test, y_pred):\n",
    "    cm = confusion_matrix(y_test, y_pred)  # Compute confusion matrix\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['No Tumor', 'glioma', 'meninglioma', 'pituatary'], yticklabels=['No Tumor', 'glioma', 'meninglioma', 'pituatary'])  # Plot confusion matrix as heatmap\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.tight_layout()\n",
    "    # plt.savefig(\"plots/confusion_matrix.png\", dpi=300)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "96Xk_PaOg5Kk"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vn-FkGrsg3mN"
   },
   "source": [
    "From here, we have started initializing variables necessary to run the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "NhO8Saw4_Drj"
   },
   "outputs": [],
   "source": [
    "# These are the 4 file directories or tumors that we study\n",
    "file_directories = [\"glioma_tumor\", \"meningioma_tumor\", \"pituitary_tumor\", \"no_tumor\"]  # Directory of the file\n",
    "\n",
    "# Classes of tumors and healthy brain mapped to numeric labels\n",
    "class_directory = {\n",
    "    'glioma': 1,\n",
    "    'meningioma': 2,\n",
    "    'pituitary': 3,\n",
    "    'healthy': 0\n",
    "}\n",
    "\n",
    "# Store each image and it's correct label\n",
    "labels = []\n",
    "\n",
    "directory = {\n",
    "    'glioma': [],\n",
    "    'meningioma': [],\n",
    "    'pituitary': [],\n",
    "    'healthy': []\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CT1Gza3_hFVt"
   },
   "source": [
    "In this snippet, we import all the images, split the images into training and testing data sets and then sets class weights based on the number of each cases we have in our data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "SZI6nzZzhAkL"
   },
   "outputs": [],
   "source": [
    "# Importing MRI Images from the directory\n",
    "training_images_directory, training_labels = import_images(file_directories, class_directory, directory, labels)\n",
    "\n",
    "# Generate Training Data:\n",
    "training_data = []\n",
    "for key in training_images_directory:\n",
    "    training_data = training_data + training_images_directory[key]\n",
    "\n",
    "# Converting Training Data from list np.array\n",
    "training_data = np.array(training_data, dtype='float32')\n",
    "training_labels = np.array(training_labels, dtype='float32')\n",
    "\n",
    "# Splitting properties\n",
    "split_ratio = 0.1\n",
    "random_int = random.randint(1, 100)\n",
    "\n",
    "# Split the data set into training and testing\n",
    "training_data, testing_data, training_labels, testing_labels = train_test_split(training_data, training_labels, test_size=split_ratio, random_state=random_int)\n",
    "\n",
    "# Based on the size of training data set, compute weights to set priorities\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight=\"balanced\",\n",
    "    classes=np.array([0, 1, 2, 3]),\n",
    "    y=training_labels\n",
    ")\n",
    "\n",
    "# Convert class weights into dictionary (0 - given the most as it's healthy brain case)\n",
    "class_weights = dict(enumerate(class_weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fuyxuEzRhT67"
   },
   "source": [
    "Before training the model, we check if everything is correct or something isn't done correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pZ7lopIDhTLX",
    "outputId": "7e17e749-5183-42bf-bd50-8671722dbea6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2937, 80, 80)\n",
      "(2937,)\n",
      "(327, 80, 80)\n",
      "(327,)\n",
      "{0: np.float64(1.6389508928571428), 1: np.float64(0.8782894736842105), 2: np.float64(0.8772401433691757), 3: np.float64(0.8998161764705882)}\n"
     ]
    }
   ],
   "source": [
    "# Check the data sets and class weights\n",
    "print(training_data.shape)\n",
    "print(training_labels.shape)\n",
    "print(testing_data.shape)\n",
    "print(testing_labels.shape)\n",
    "print(class_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y39bj4JOhfg6"
   },
   "source": [
    "Finally, we run the model that we have built and get all the metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "n3_gorVphav_",
    "outputId": "a01f1685-7e66-4888-ebe0-7e5590e31b44"
   },
   "outputs": [],
   "source": [
    "# 2D CNN: Finally, calling the function to train and then validate our model\n",
    "history_mri, testing_pred_prob_mri = train_and_evaluate_model(build_model_2D_cnn, training_data, training_labels, testing_data, testing_labels, class_weights)\n",
    "\n",
    "# 1D CNN (V2): Finally, calling the function to train and then validate our model\n",
    "# history_mri, testing_pred_prob_mri = train_and_evaluate_model(build_model_1D_cnn, training_data, training_labels, testing_data, testing_labels, class_weights)\n",
    "\n",
    "# print(testing_pred_prob_mri)\n",
    "\n",
    "# Generating the prediction array (getting the most high probability value label from each case)\n",
    "predictions = []\n",
    "for probs in testing_pred_prob_mri:\n",
    "  max_ind = 0\n",
    "  val = probs[0]\n",
    "  for i in range(1, len(probs)):\n",
    "    if probs[i] > val:\n",
    "      max_ind = i\n",
    "      val = probs[i]\n",
    "\n",
    "  predictions.append(max_ind)\n",
    "\n",
    "predictions = np.array(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O51V8SRvhpUk"
   },
   "source": [
    "Time for some plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BDvn2p2khmzT"
   },
   "outputs": [],
   "source": [
    "# Plotting the metrics received from running the models.\n",
    "plot_confusion_matrix(testing_labels, predictions)\n",
    "plot_training_loss(history_mri)\n",
    "plot_training_history(history_mri)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
